---
title: "DS_CP"
author: "Devang"
date: "1/3/2022"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Macroeconomic Trends Prediction
## Trend 1 : Bitcoin Price Prediction

```{r}
BTC<-read.csv("bitcoin_dataset.csv")
library(caret)
BTC<-na.omit(BTC)
library(corrplot)
cor <- cor(BTC[,2:12])
corrplot(cor, method = "pie")
```

```{r}
mod<-lm(btc_market_price~btc_market_cap, data=BTC)
mod1<-lm(btc_market_price~btc_estimated_transaction_volume_usd, data=BTC)
summary(mod)
summary(mod1)
```

```{r}
plot(BTC$btc_market_price,BTC$btc_market_cap)
plot(BTC$btc_market_price,BTC$btc_estimated_transaction_volume_usd)
```

```{r}
set.seed(1)
train.index<-sample(1:nrow(BTC),0.70*nrow(BTC), replace=FALSE)
train <- BTC[train.index, ]
test  <- BTC[-train.index,]
```

```{r}
# Training the model on train dataset and predicting on test.
model1 <- lm(btc_market_price~btc_trade_volume , train)
summary(model1)
p1 <- predict(model1,test)
head(p1)
error1 <- p1 - test[["btc_market_price"]]
sqrt(mean(error1^2))
plot(p1)
# RMSE=510.7136
```

```{r}
model2 <- lm(btc_market_price~btc_market_cap, train)
summary(model2)
p2 <- predict(model2,test)
head(p2)
error2 <- p2 - test[["btc_market_price"]]
sqrt(mean(error2^2))
plot(p2)
```

```{r}
library(FactoMineR)
pc1<-PCA(BTC[,2:24],scale.unit = TRUE, ncp = 23, graph = TRUE)
summary(pc1)
a<-dimdesc(pc1,axes = c(1:2))
a$Dim.1
a$Dim.2
```

```{r}
pc<-prcomp(BTC[,c("btc_market_price","btc_miners_revenue")], center = T, scale=T)
summary(pc)
head(pc$x)
```

```{r}
model3<-lm(btc_market_price~btc_n_transactions_total, data=BTC)
summary(model3)
p3 <- predict(model3,test)
error3 <- p3 - test[["btc_market_price"]]
sqrt(mean(error3^2))
```

```{r}
model4<-lm(btc_market_price~btc_n_transactions_total+btc_avg_block_size+btc_difficulty+btc_output_volume, data=BTC)
summary(model4)
p4 <- predict(model4,test)
error4 <- p4 - test[["btc_market_price"]]
sqrt(mean(error4^2))
```

## Trend 2 - Healthcare Finances Prediction

## Setting up the environment and data import
```{r import, message=FALSE, warning=FALSE, paged.print=TRUE}
library(ggplot2)
library(dplyr)
library(Hmisc)
library(cowplot)
library(WVPlots)
set.seed(123)
Data <- read.csv("insurance.csv")
sample_n(Data, 5)
```

## Exploratory Data Analysis

```{r EDA, message=FALSE, warning=FALSE, paged.print=TRUE}
x <- ggplot(Data, aes(age, charges)) +
  geom_jitter(color = "blue", alpha = 0.5) +
    theme_light()

y <- ggplot(Data, aes(bmi, charges)) +
  geom_jitter(color = "green", alpha = 0.5) +
  theme_light()

p <- plot_grid(x, y) 
title <- ggdraw() + draw_label("1. Correlation between Charges and Age / BMI", fontface='bold')
plot_grid(title, p, ncol=1, rel_heights=c(0.1, 1))

x <- ggplot(Data, aes(sex, charges)) +
  geom_jitter(aes(color = sex), alpha = 0.7) +
  theme_light()

y <- ggplot(Data, aes(children, charges)) +
  geom_jitter(aes(color = children), alpha = 0.7) +
  theme_light()

p <- plot_grid(x, y) 
title <- ggdraw() + draw_label("2. Correlation between Charges and Sex / Children covered by insurance", fontface='bold')
plot_grid(title, p, ncol=1, rel_heights=c(0.1, 1))

x <- ggplot(Data, aes(smoker, charges)) +
  geom_jitter(aes(color = smoker), alpha = 0.7) +
  theme_light()

y <- ggplot(Data, aes(region, charges)) +
  geom_jitter(aes(color = region), alpha = 0.7) +
  theme_light()

p <- plot_grid(x, y) 
title <- ggdraw() + draw_label("3. Correlation between Charges and Smoker / Region", fontface='bold')
plot_grid(title, p, ncol=1, rel_heights=c(0.1, 1))
```

* **Plot 1**: As Age and BMI go up Charges for health insurance also trends up.

* **Plot 2**: No obvious connection between Charges and Age. Charges for insurance with 4-5 children covered seems to go down.

* **Plot 3**: Charges for Smokers are higher for non-smokers.No obvious connection between Charges and Region.

## Linear Regression Model
### Preparation and splitting the data
```{r prep, message=FALSE, warning=FALSE, paged.print=TRUE}
n_train <- round(0.8 * nrow(Data))
train_indices <- sample(1:nrow(Data), n_train)
Data_train <- Data[train_indices, ]
Data_test <- Data[-train_indices, ]
formula_0 <- as.formula("charges ~ age + sex + bmi + children + smoker + region")
```

### Train and Test the Model
```{r model_0, message=FALSE, warning=FALSE, paged.print=TRUE}
model_0 <- lm(formula_0, data = Data_train)
summary(model_0)
#Saving R-squared
r_sq_0 <- summary(model_0)$r.squared
#predict data on test set
prediction_0 <- predict(model_0, newdata = Data_test)
#calculating the residuals
residuals_0 <- Data_test$charges - prediction_0
#calculating Root Mean Squared Error
rmse_0 <- sqrt(mean(residuals_0^2))
```

As we can see, summary of a model showed us that some of the variable are not significant (*sex*), while *smoking* seems to have a huge influence on *charges*. Training a model without non-significant variables and check if performance can be improved.

### Train and Test New Model
```{r model_1, message=FALSE, warning=FALSE, paged.print=TRUE}
formula_1 <- as.formula("charges ~ age + bmi + children + smoker + region")

model_1 <- lm(formula_1, data = Data_train)
summary(model_1)
r_sq_1 <- summary(model_1)$r.squared

prediction_1 <- predict(model_1, newdata = Data_test)

residuals_1 <- Data_test$charges - prediction_1
rmse_1 <- sqrt(mean(residuals_1^2))
```

### Compare the models
```{r comparison, message=FALSE, warning=FALSE, paged.print=TRUE}
print(paste0("R-squared for first model:", round(r_sq_0, 4)))
print(paste0("R-squared for new model: ", round(r_sq_1, 4)))
print(paste0("RMSE for first model: ", round(rmse_0, 2)))
print(paste0("RMSE for new model: ", round(rmse_1, 2)))
```

As we can see, performance is quite similar between two models so we will keep the new model since it's a little bit simpler.

### Model Performance
```{r performance, message=FALSE, warning=FALSE, paged.print=TRUE}
Data_test$prediction <- predict(model_1, newdata = Data_test)
ggplot(Data_test, aes(x = prediction, y = charges)) + 
  geom_point(color = "blue", alpha = 0.7) + 
  geom_abline(color = "red") +
  ggtitle("Prediction vs. Real values")

Data_test$residuals <- Data_test$charges - Data_test$prediction

ggplot(data = Data_test, aes(x = prediction, y = residuals)) +
  geom_pointrange(aes(ymin = 0, ymax = residuals), color = "blue", alpha = 0.7) +
  geom_hline(yintercept = 0, linetype = 3, color = "red") +
  ggtitle("Residuals vs. Linear model prediction")

ggplot(Data_test, aes(x = residuals)) + 
  geom_histogram(bins = 15, fill = "blue") +
  ggtitle("Histogram of residuals")

GainCurvePlot(Data_test, "prediction", "charges", "Model")
```

We can see the errors in the model are close to zero so model predicts quite well.
### Applying on new data
## Trend 3 - House Price Prediction

```{r}
library(ggplot2)
library(GGally)
options(repr.plot.width = 12, repr.plot.height = 8)
```

```{r}
data <- read.csv(file = 'data.csv')
head(data)
```

```{r}
print(paste("Number of records: ", nrow(data)))
print(paste("Number of features: ", ncol(data)))
```

```{r}
summary(data)
```

```{r}
colnames(data)
```

```{r}
unique(data$city)
```

```{r}
maindf <- data[,c("price","bedrooms","sqft_living","floors",
                  "sqft_lot", "condition", "view", "yr_built")]
head(maindf)
```

```{r}
sum(is.na(maindf)) 
```

```{r}
maindf
```

```{r}
cor(maindf)
```

```{r}
ggcorr(maindf, name="corr", label=T)
```

```{r}
pairs(~bedrooms + sqft_living + floors + condition, data = maindf,
       main = "Scatterplot Matrix")
```

```{r}
par(mfrow=c(2, 3))  # divide graph area in 2 columns
boxplot(maindf$bedrooms, main="Bedrooms")
boxplot(maindf$sqft_living, main="sqft_living")
boxplot(maindf$floors, main="floors")
boxplot(maindf$condition, main="condition")
boxplot(maindf$view, main="view")
boxplot(maindf$oldbuilt, main="oldbuilt")
```

```{r}
library(e1071)

par(mfrow=c(2, 3)) 

plot(density(maindf$bedrooms), main="Density Plot: Bedrooms", ylab="Frequency",
     sub=paste("Skewness:", round(e1071::skewness(maindf$bedrooms), 2)))  
polygon(density(maindf$bedrooms), col="green")

plot(density(maindf$sqft_living), main="Density Plot: sqft_living", ylab="Frequency",
     sub=paste("Skewness:", round(e1071::skewness(maindf$sqft_living), 2)))  
polygon(density(maindf$sqft_living), col="orange")

plot(density(maindf$sqft_lot), main="Density Plot: sqft_lot", ylab="Frequency",
     sub=paste("Skewness:", round(e1071::skewness(maindf$sqft_lot), 2)))  
polygon(density(maindf$sqft_lot), col="green")

plot(density(maindf$condition), main="Density Plot: condition", ylab="Frequency",
     sub=paste("Skewness:", round(e1071::skewness(maindf$condition), 2)))  
polygon(density(maindf$condition), col="orange")

plot(density(maindf$floors), main="Density Plot: floors", ylab="Frequency",
     sub=paste("Skewness:", round(e1071::skewness(maindf$floors), 2)))  
polygon(density(maindf$floors), col="green")

plot(density(maindf$oldbuilt), main="Density Plot: oldbuilt", ylab="Frequency",
     sub=paste("Skewness:", round(e1071::skewness(maindf$oldbuilt), 2)))  
polygon(density(maindf$oldbuilt), col="orange")
```

```{r}
ggplot(maindf,aes(y=price,x=sqft_living)) +
       geom_point() + 
        xlim(0, 9000) +
        ylim(0, 5000000) +
        geom_smooth(formula = y ~ x,method="lm")
```

```{r}
sn <- sample(1:nrow(maindf), size = nrow(maindf)*0.8) 
train_df <- maindf[sn,]             
test_df <- maindf[-sn,]
nrow(train_df)
nrow(test_df)
```

```{r}
head(train_df)
head(test_df) 
```

```{r}
linearmodel = lm(price~bedrooms + sqft_living + floors + sqft_lot + condition + view + oldbuilt,
                 data = train_df)
summary(linearmodel) 
```

```{r}
pred <- predict(linearmodel, newdata = test_df)
pred <- as.data.frame(pred)
head(pred)
```

```{r}
rate <- data.frame(test_df$price, pred)
```

```{r}
rate
````